; Phllama PHP Extension Configuration
; High-performance AI inference extension for PHP

; Basic extension configuration
extension=phllama.so

; Hardware Configuration
; =====================

; GPU Mode: 0=CPU_ONLY, 1=SINGLE_GPU, 2=DUAL_GPU, -1=AUTO (default: -1)
phllama.gpu_mode = -1

; Number of layers to offload to GPU (-1 = auto-detect all layers)
phllama.gpu_layers = -1

; Primary GPU device for single GPU mode (default: 0)
phllama.main_gpu = 0

; CPU threads (-1 = auto-detect optimal, default: -1)
phllama.cpu_threads = -1

; Memory Configuration
; ===================

; Use memory mapping for model files (recommended: true)
phllama.use_mmap = true

; Lock model in memory (not recommended for large models)
phllama.use_mlock = false

; Dual GPU Configuration
; ======================

; Enable tensor splitting for dual GPU setups
phllama.tensor_split_enabled = true

; Tensor split ratios for dual GPUs (comma-separated, must sum to 1.0)
; Example: 0.5,0.5 for equal split or 0.6,0.4 for 60/40 split
phllama.tensor_split = "0.5,0.5"

; Performance Tuning
; ==================

; Context window size (default: 2048)
phllama.context_size = 2048

; Batch size for processing (default: 512)
phllama.batch_size = 512

; Model Management
; ===============

; Default ollama models directory (leave empty for auto-detection)
phllama.models_directory = ""

; Cache Configuration
; ==================

; Auto-clear cache after this many generations (0 = disabled)
phllama.auto_clear_cache = 0

; Logging and Debugging
; ====================

; Enable performance monitoring (default: false)
phllama.enable_monitoring = false

; Log GPU memory usage (default: false)
phllama.log_gpu_memory = false

; Hardware-Specific Optimizations
; ===============================

; For dual RTX 4090 setup - optimal configurations:
; phllama.gpu_mode = 2
; phllama.gpu_layers = -1
; phllama.tensor_split = "0.5,0.5"
; phllama.cpu_threads = 14
; phllama.use_mmap = true
; phllama.use_mlock = false

; For single RTX 4090 setup:
; phllama.gpu_mode = 1
; phllama.gpu_layers = -1
; phllama.main_gpu = 0
; phllama.cpu_threads = 8

; For CPU-only (dual Xeon):
; phllama.gpu_mode = 0
; phllama.gpu_layers = 0
; phllama.cpu_threads = 16
; phllama.use_mmap = true